{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <font color=\"#332E3C\"><b>Cost Prediction for Logistic Company</b></font>\n\n>**Team MU**\n>\n>- Camilo Florez        (C0888810)\n>- Syed Zumair          (C0885081)\n>- Karman Singh Bains   (C0885084)\n>- Mahaveersinh Chauhan (C0884854)\n>- Joel Castro           (C0883863)\n>- Manpreet Singh        (C0884727)","metadata":{}},{"cell_type":"markdown","source":"\n## Table of contents <a class=\"anchor\" id=\"0.1\"></a>\n\n1. [Introduction](#1)\n2. [Business Case Evaluation](#2)\n3. [Data Identification](#3)<br>\n4. [Prepare for Analysis](#4)<br>\n    4.1 [Libraries](#4.1)<br>\n    4.2 [General Functions](#4.2)\n5. [Data Extraction](#5)\n6. [Data Validation & Cleansing](#6)<br>\n    6.1 [Duplicate Values](#6.1)<br>\n    6.2 [Unique Values & Typographic Error](#6.2)<br>\n    6.3 [Outliers](#6.3)<br>\n    6.4 [Missing Values](#6.4)<br>\n7. [Exploration Data Analysis](#7)\n8. [Machine Learning Techniques](#8)<br>\n    8.1[Split](#8.1)<br>\n    8.2[Linear Regression](#8.2)<br>\n    8.3[Random Forest Regressor](#8.3)<br>\n    8.4[Gradient Boosting Regressor](#8.4)<br>\n    8.5[XGB Booster](#8.5)\n9. [Conclusion](#9)","metadata":{}},{"cell_type":"markdown","source":"[Back to Table of Contents](#0.1)\n## Introduction <a class=\"anchor\" id=\"1\"></a>\nAs businesses increasingly rely on efficient and timely delivery of goods, logistics companies seek ways to optimize their delivery networks. They require accurate predictions of the costs associated with delivering packages across their network to achieve this. In this scenario, a logistics company has shared a dataset of network-wide deliveries and their corresponding trip costs. The task at hand is to build a model that can accurately predict the cost of deliveries, enabling the company to optimize its network and improve its overall efficiency.","metadata":{}},{"cell_type":"markdown","source":"[Back to Table of Contents](#0.1)\n## Business Case Evaluation <a class=\"anchor\" id=\"2\"></a>\n\n#### Problem definition\nThe prediction of the delivery cost is requested.\n\nEvaluation Metric : RMSE Score\n\nThe file should contain a header and have the following format:\ntrip,cost\nt333282728025, 23.3433","metadata":{}},{"cell_type":"markdown","source":"[Back to Table of Contents](#0.1)\n## Data Identification <a class=\"anchor\" id=\"3\"></a>\nThe data contains information related to Logistic Company.<br><br>\n\n|COLUMN| DESCRIPTION|\n|:--|:--|\n|Trip|Trip id|\n|Date|Date, when trip was made|\n|DayPart|Day or Night|\n|ExWeatherTag|Heat/Snow|\n|OriginLocation|Source location|\n|DestinationLocation|Destination location|\n|Distance|distance traveled|\n|Type|Expedited or Not|\n|Weight|weight carried by carriers|\n|PackageType |Type of packages|\n|carrier|Name of the carriers|\n\n\nDatasets\nTraining data –Training dataset contains 38999 observations.\nActual responses (labels) are in the variable called ‘Cost’ Testing data – contains 802 observations with 11 variables and no labels","metadata":{}},{"cell_type":"markdown","source":"### Metadata \nKaggel Competition<br>\n**File descriptions**<br>\ntrain.csv - the training Dataset<br>\ntest.csv - the test Dataset","metadata":{}},{"cell_type":"markdown","source":"[Back to Table of Contents](#0.1)\n## Prepare for Data Analysis <a class=\"anchor\" id=\"4\"></a>\n\n#### Import Libraries <a class=\"anchor\" id=\"4.1\"></a>","metadata":{}},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport datetime\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nimport plotly.express as px\nimport time\nfrom tqdm import tqdm\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn import metrics\nfrom scipy import stats\nimport seaborn as sns\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import norm, skew\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\n\n# Ignore warning messages\nwarnings.filterwarnings('ignore')\n\n# More columns the line\npd.set_option(\"display.max_rows\", 500)\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.width\", 1000)\n\n# static variables\norder_months = {\"January\":1, \"February\":2, \"March\":3, \"April\":4, \"May\":5, \"June\":6, \"July\":7, \"August\":8, \"September\":9, \"October\":10,\n                \"November\":11, \"December\":12}\n","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:33.231414Z","end_time":"2023-04-06T00:25:33.749339Z"},"execution":{"iopub.status.busy":"2023-04-09T23:28:06.747655Z","iopub.execute_input":"2023-04-09T23:28:06.748072Z","iopub.status.idle":"2023-04-09T23:28:06.759851Z","shell.execute_reply.started":"2023-04-09T23:28:06.748031Z","shell.execute_reply":"2023-04-09T23:28:06.75833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### General Functions <a class=\"anchor\" id=\"4.2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Class created for creating the graph and define the basic functionality of the graph.","metadata":{}},{"cell_type":"code","source":"class Graphs:\n    def __init__(self, title, x_rotation=None, x_fontsize=None, bar_rotation=None, bar_fontsize=None,\n                 column_huge_name=None, legend_loc='upper left'):\n        \"\"\"Define Basic Functionality of the Graphs\n\n        Args:\n            title (String): Title of the Graph\n            x_rotation (Int, optional): X Rotation of the Graph. Defaults to None.\n            x_fontsize (Int, optional): Y Rotation of the Graph. Defaults to None.\n            bar_rotation (Int, optional): Bar Rotation of the Graph. Defaults to None.\n            bar_fontsize (Int, optional): Font size of the Graph. Defaults to None.\n            column_huge_name (String, optional): Column Huge Name of the Graph. Defaults to None.\n            legend_loc (String, optional): Legend Location of the Graph. Defaults to 'upper left'.\n        \"\"\"\n        self.title = title\n        self.x_rotation = x_rotation\n        self.x_fontsize = x_fontsize\n        self.bar_rotation = bar_rotation\n        self.bar_fontsize = bar_fontsize\n        self.column_huge_name = column_huge_name\n        self.legend_loc = legend_loc\n        \n    def bar_plot(self, column_x, column_y, column_x_name, column_y_name, column_hue=None, order_x=None):\n        \"\"\"Plot Bar Graph\n        Args:\n            column_x (DataFrame): X-Bar of the Graph\n            column_y (DataFrame): Y-Bar of the Graph\n            column_x_name (String): Name of the X-Bar\n            column_y_name (String): Name of the Y-Bar\n            column_hue (String, optional): Column Hue of the Graph. Defaults to None.\n            order_x (List of the String, optional): Order to plot the categorical levels in the Graph. Defaults to None.\n        \"\"\"\n        fig, ax = plt.subplots(figsize=(15, 7))\n        sns.barplot(x=column_x, y=column_y, hue=column_hue, order=order_x, ax=ax, errorbar=None)\n        [ax.bar_label(cont, padding=3, fmt='%d', fontsize=self.bar_fontsize, rotation=self.bar_rotation) for cont in\n         ax.containers]\n        plt.xlabel(column_x_name)\n        plt.ylabel(column_y_name)\n        if column_hue is not None and self.column_huge_name is not None:\n            plt.legend(title=self.column_huge_name, loc=self.legend_loc)\n        plt.title(self.title, fontsize=12).set_position([0.5, 1.05])\n        plt.xticks(rotation=self.x_rotation, fontsize=12)\n        plt.show()\n\n    @staticmethod\n    def distribution_plot(data_filter, cols_num):\n        \"\"\"Plot the Distribution Graph\n\n        Args:\n            data_filter (DataFrame): Data for the Graph\n            cols_num (List of String): Columns list for the Graph\n        \"\"\"\n        fig, axes = plt.subplots(ncols=3, figsize=(16, 4))\n        fig.subplots_adjust(hspace=1)\n        for i, col in enumerate(cols_num):\n            sns.kdeplot(data=data_filter,x=col, shade=True, kernel='gau',color=\"Red\", ax=axes[i], legend=True)\n            axes[i].set_title(col)\n            fig.tight_layout()\n\n    def heat_map_corr(self, data):\n        \"\"\"Correlation heat map of the graph\n\n        Args:\n            data (DataFrame): Data for the Graph\n        \"\"\"\n        plt.figure(figsize=(12, 8))\n        plt.title(self.title, y=1.05,size=12)\n        sns.heatmap(data, square=True, cmap='RdYlBu', linecolor=\"white\", annot=True,annot_kws={\"size\": 7})\n        plt.tight_layout()\n        plt.show()\n\n    def heatmap_nulls(self, data):\n        \"\"\"Graph of the Null value\n\n        Args:\n            data (DataFrame): Data for the Graph\n        \"\"\"\n        ax = plt.axes()\n        ax.xaxis.tick_top()\n        sns.heatmap(data.isnull(), cbar=False, ax=ax)\n        plt.title(self.title)\n        plt.xlabel(\"Column\")\n        plt.ylabel(\"Quantity\")\n        plt.xticks(fontsize=9)\n        plt.yticks(fontsize=6)\n        plt.show() \n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:40.618185Z","iopub.execute_input":"2023-04-09T23:02:40.618692Z","iopub.status.idle":"2023-04-09T23:02:40.634953Z","shell.execute_reply.started":"2023-04-09T23:02:40.618665Z","shell.execute_reply":"2023-04-09T23:02:40.633601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created for making the dummy variable of the categorical data.","metadata":{}},{"cell_type":"code","source":"def merge_dummy_data(data, column):\n    \"\"\"Create Dummy variable of the data and merge to original dataframe\n\n    Args:\n        data (DataFrame): Original Data\n        column (String): Categorical Column of the Data\n\n    Returns:\n        DataFrame: Return Merged DataFrame\n    \"\"\"\n    dummies = pd.get_dummies(data[column], prefix=column, dummy_na=False)\n    merged_imputed = pd.concat([data, dummies], axis=1)\n    merged_imputed.drop([column], axis=1, inplace=True)\n    return merged_imputed","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:40.982189Z","iopub.execute_input":"2023-04-09T23:02:40.982705Z","iopub.status.idle":"2023-04-09T23:02:40.988647Z","shell.execute_reply.started":"2023-04-09T23:02:40.982676Z","shell.execute_reply":"2023-04-09T23:02:40.987371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created to get the missing values","metadata":{}},{"cell_type":"code","source":"def missing_value(train):\n   \"\"\"Check Missing Value\n   Args:\n       train (DataFrame): Data to check missing value\n\n   Returns:\n       DataFrame: Return DataFrame which has summery of the missing value\n   \"\"\"\n   obs = train.isnull().sum().sort_values(ascending = False)\n   percent = round(train.isnull().sum().sort_values(ascending = False)/len(train)*100, 2)\n   missing = pd.concat([obs, percent], axis = 1,keys= ['Number of Observations', 'Percent'])\n   return missing","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:41.301047Z","iopub.execute_input":"2023-04-09T23:02:41.301606Z","iopub.status.idle":"2023-04-09T23:02:41.307148Z","shell.execute_reply.started":"2023-04-09T23:02:41.301533Z","shell.execute_reply":"2023-04-09T23:02:41.305967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created to identify the outliers","metadata":{}},{"cell_type":"code","source":"def iden_outliers(df,cols_num):\n    \"\"\"Identify The Outlier of the Data\n\n    Args:\n        df (DataFrame): Data to find the outlier\n        cols_num (String): Find the outlier for that particular column\n    \"\"\"\n    fig, ax = plt.subplots(nrows=3,ncols=1,figsize=(12,6))\n    fig.subplots_adjust(hspace=1)\n    for i, col in enumerate(cols_num):\n        sns.boxplot(x=col,data=df,ax=ax[i])\n        #ax[i].set_title(col)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:41.568735Z","iopub.execute_input":"2023-04-09T23:02:41.569084Z","iopub.status.idle":"2023-04-09T23:02:41.576212Z","shell.execute_reply.started":"2023-04-09T23:02:41.569055Z","shell.execute_reply":"2023-04-09T23:02:41.574556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created to analysis outliers","metadata":{}},{"cell_type":"code","source":"def outliers_analysis(df):\n    \"\"\"Get the Mean, Mode, Median and Standard Deviation of the data\n\n    Args:\n        df (DataFrame): Data that need to analysis the outlier\n    \"\"\"\n    mean = np.mean(df_train_date[df])\n    median = np.median(df_train_date[df])\n    mode = stats.mode(df_train_date[df])\n    std_dev = np.std(df_train_date[df])\n    print(\"mean:\",mean)\n    print(\"median:\",median)\n    print(\"mode:\",mode)\n    print(\"standar desviation:\",std_dev)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:41.886457Z","iopub.execute_input":"2023-04-09T23:02:41.886798Z","iopub.status.idle":"2023-04-09T23:02:41.893713Z","shell.execute_reply.started":"2023-04-09T23:02:41.88677Z","shell.execute_reply":"2023-04-09T23:02:41.89293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created for removing outlier using IQR algorithm","metadata":{}},{"cell_type":"code","source":"def iqr_outliers(df):\n    \"\"\"Apply the IQR outlier algorithm to remove the outlier\n\n    Args:\n        df (DataFrame): Data that need to remove outlier\n\n    Returns:\n        DataFramr: Filterd DataFrame\n    \"\"\"\n    out = []\n    if not any(df):\n        return out\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3 - q1\n    lower_tail = q1 - 1.5 * iqr\n    upper_tail = q3 + 1.5 * iqr\n    for i in df:\n        if i > upper_tail or i < lower_tail:\n            out.append(i)\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:42.214267Z","iopub.execute_input":"2023-04-09T23:02:42.215212Z","iopub.status.idle":"2023-04-09T23:02:42.222614Z","shell.execute_reply.started":"2023-04-09T23:02:42.215171Z","shell.execute_reply":"2023-04-09T23:02:42.221346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created to transfor the categorical columns to numerical columns","metadata":{}},{"cell_type":"code","source":"def format_data_etiqueta(df,c):\n    \"\"\"Apply Label Encoder\n\n    Args:\n        df (DataFrame): Dataset\n        c (Column): Column that needs to apply label encoder\n    \"\"\"\n    le = LabelEncoder()\n    df[c]= le.fit_transform(df[c])\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:42.543959Z","iopub.execute_input":"2023-04-09T23:02:42.544319Z","iopub.status.idle":"2023-04-09T23:02:42.550168Z","shell.execute_reply.started":"2023-04-09T23:02:42.544291Z","shell.execute_reply":"2023-04-09T23:02:42.549157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Linear Regression Fuction","metadata":{}},{"cell_type":"code","source":"def linearregresion (X_train,y_train):\n    \"\"\"Apply Linear Regression Alogrithm\n\n    Args:\n        X_train (DataFrame): Train Dataset\n        y_train (DataFrame): Target Dataset of the Train Dataset\n\n    Returns:\n        Object: Object of the Linear Regression Algorithm\n    \"\"\"\n    reg = LinearRegression().fit(X_train, y_train)\n    return reg","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:42.944335Z","iopub.execute_input":"2023-04-09T23:02:42.94489Z","iopub.status.idle":"2023-04-09T23:02:42.95118Z","shell.execute_reply.started":"2023-04-09T23:02:42.94486Z","shell.execute_reply":"2023-04-09T23:02:42.949073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random forest regressor function","metadata":{}},{"cell_type":"code","source":"def random_forest_regressor(x_train,y_train):\n    \"\"\"Apply Random Forest Regression Algorithm\n\n    Args:\n        X_train (DataFrame): Train Dataset\n        y_train (DataFrame): Target Dataset of the Train Dataset\n\n    Returns:\n        Object: Object of the Linear Regression Algorithm\n    \"\"\"\n    try:\n        rfr = RandomForestRegressor(random_state=0)\n        rfr.fit(x_train, y_train)\n        return rfr\n    except Exception as ex:\n        return None","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:43.303635Z","iopub.execute_input":"2023-04-09T23:02:43.304344Z","iopub.status.idle":"2023-04-09T23:02:43.310378Z","shell.execute_reply.started":"2023-04-09T23:02:43.304304Z","shell.execute_reply":"2023-04-09T23:02:43.308891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created for get the data metrics ","metadata":{}},{"cell_type":"code","source":"def model_error(y_test,y_pred):\n    \"\"\"Check the model performance\n\n    Args:\n        y_test (DataFrame): Test Dataset\n        y_pred (DataFrame): Predicted Dataset\n    \"\"\"\n    r2 = r2_score(y_test, y_pred)\n    mae = mean_absolute_error(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    performance = r2 - mse\n    print(\"performance: \", performance)\n    print(\"MAE: \", mae)\n    print(\"R2: \", r2)\n    print(\"MSE: \", mse)\n    print(\"RMSE: \", np.sqrt(mse))","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:43.716561Z","iopub.execute_input":"2023-04-09T23:02:43.716932Z","iopub.status.idle":"2023-04-09T23:02:43.723764Z","shell.execute_reply.started":"2023-04-09T23:02:43.716905Z","shell.execute_reply":"2023-04-09T23:02:43.722531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created to find the best hyperparameters using GridSearchCV","metadata":{}},{"cell_type":"code","source":"def grid_search_cv(model: object, param_grid: dict, X_train: object, y_train: object) -> object:\n    \"\"\"Get the best hyperparameter to the model\n\n    Args:\n        model (object): initial model\n        param_grid (dict): hyperparameters dict\n        X_train (object): Train Dataset\n        y_train (object): Target Dataset\n\n    Returns:\n        object: Model with Best estimator\n    \"\"\"\n    gs = GridSearchCV(model, param_grid, cv=3, scoring=\"neg_mean_squared_error\", refit=True, n_jobs= -1) \n    gs.fit(X_train, y_train)\n    best_params = gs.best_params_\n    best_estimators = gs.best_estimator_\n\n    # Print the best params\n    print('Best Params: ', best_params)\n    # Print the best estimators\n    print('Best Estimators', best_estimators)\n    # Print the accuracy score\n    print('Accuracy Score:', best_estimators.score(X_train, y_train))\n    \n    return best_estimators","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:44.021559Z","iopub.execute_input":"2023-04-09T23:02:44.021903Z","iopub.status.idle":"2023-04-09T23:02:44.030545Z","shell.execute_reply.started":"2023-04-09T23:02:44.021875Z","shell.execute_reply":"2023-04-09T23:02:44.028348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created to find best hyperparameter using RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"def randomized_search_cv(model: object, param_grid: dict,  X_train: object, y_train: object, n_iter: int = 20, verbose: int = 2) -> object:\n    \"\"\"Get the best hyperparameter to the model\n\n    Args:\n        model (object): initial model\n        param_grid (dict): hyperparameters dict\n        X_train (object): Train Dataset\n        y_train (object): Target Dataset\n        n_iter (int, optional): number of iterations. Defaults to 25.\n        verbose (int, optional): verbose. Defaults to 2.\n\n    Returns:\n        object: model with best estimators\n    \"\"\"\n    rnd_search_cv = RandomizedSearchCV(model, param_grid, n_iter=n_iter,scoring='neg_mean_absolute_error',verbose=verbose,cv = 3, n_jobs=-1)\n    rnd_search_cv.fit(X_train, y_train)\n    # Get the best estimator on Randomized SearchCV\n    best_estimators = rnd_search_cv.best_estimator_\n\n    # Print the best estimators\n    print('Best Estimators', best_estimators)    \n    # Print the accuracy score\n    print('Accuracy Score:', rnd_search_cv.score(X_train, y_train))\n    \n    return best_estimators","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:31:17.166071Z","iopub.execute_input":"2023-04-09T23:31:17.167576Z","iopub.status.idle":"2023-04-09T23:31:17.174902Z","shell.execute_reply.started":"2023-04-09T23:31:17.167513Z","shell.execute_reply":"2023-04-09T23:31:17.173665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created to identify the importance columns (Model with coef)","metadata":{}},{"cell_type":"code","source":"def feature_importance_lineal(model: object, X_train: object) -> None:\n    \"\"\"Get the list of feature importance\n\n    Args:\n        model (object): initial model\n        X_train (object): Train Dataset\n    \"\"\"\n    for name, score in sorted(zip(X_train.columns, model.coef_), key=lambda x: x[1], reverse=True):\n        if score == 0:\n            break\n        print(\"Column:\", name, \"\\t Score:\", score)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:44.711828Z","iopub.execute_input":"2023-04-09T23:02:44.712192Z","iopub.status.idle":"2023-04-09T23:02:44.718229Z","shell.execute_reply.started":"2023-04-09T23:02:44.712166Z","shell.execute_reply":"2023-04-09T23:02:44.71725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function created to identify the importance columns (Model with feature importances)","metadata":{}},{"cell_type":"code","source":"def feature_importance(model: object, X_train: object) -> None:\n    \"\"\"Get the list of feature importance\n\n    Args:\n        model (object): initial model\n        X_train (object): Train Dataset\n    \"\"\"\n    for name, score in sorted(\n            zip(X_train.columns, model.feature_importances_), key=lambda x: x[1], reverse=True): \n        if score == 0:\n            break\n        print(\"Column:\", name, \"\\t Score:\", score)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:45.040943Z","iopub.execute_input":"2023-04-09T23:02:45.041504Z","iopub.status.idle":"2023-04-09T23:02:45.048128Z","shell.execute_reply.started":"2023-04-09T23:02:45.041472Z","shell.execute_reply":"2023-04-09T23:02:45.046159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to export the results","metadata":{}},{"cell_type":"code","source":"def export_result (y,df,name):\n    \"\"\"Save CSV File of the predicted result\n\n    Args:\n        y (DataFrame): Predicted Value of the Data\n        df (DataFrame): Original Test DataFrame\n        name (String): Name of the CSV File\n    \"\"\"\n    result = pd.DataFrame(y, columns=[\"cost\"])\n    df_test_target = pd.concat([result, df[\"trip\"]], axis=1)\n    test = df_test_target[[\"trip\", \"cost\"]]\n    test.to_csv(name + \".csv\", index=0)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:45.369945Z","iopub.execute_input":"2023-04-09T23:02:45.370273Z","iopub.status.idle":"2023-04-09T23:02:45.377932Z","shell.execute_reply.started":"2023-04-09T23:02:45.370246Z","shell.execute_reply":"2023-04-09T23:02:45.375553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function to get the Epoch time","metadata":{}},{"cell_type":"code","source":"def date_to_epoch(date):\n    \"\"\"Epoch Time\n\n    Args:\n        date (object): Date parameter\n   \"\"\"\n    epoch = datetime.utcfromtimestamp(0)\n    return int((date - epoch).total_seconds())","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:45.695274Z","iopub.execute_input":"2023-04-09T23:02:45.695592Z","iopub.status.idle":"2023-04-09T23:02:45.70342Z","shell.execute_reply.started":"2023-04-09T23:02:45.695565Z","shell.execute_reply":"2023-04-09T23:02:45.701529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### [Back to Table of Contents](#0.1)\n## Data Extraction <a class=\"anchor\" id=\"5\"></a>\nRead the train and test Datasets.","metadata":{}},{"cell_type":"code","source":"# Reading the training and test dataset\ndf_train = pd.read_csv('/kaggle/input/cost-prediction-for-logistic-company-2023w-aml1413/train.csv')\ndf_test = pd.read_csv('/kaggle/input/cost-prediction-for-logistic-company-2023w-aml1413/test.csv')","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:33.647956Z","end_time":"2023-04-06T00:25:33.830435Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:46.039591Z","iopub.execute_input":"2023-04-09T23:02:46.039946Z","iopub.status.idle":"2023-04-09T23:02:46.096614Z","shell.execute_reply.started":"2023-04-09T23:02:46.03992Z","shell.execute_reply":"2023-04-09T23:02:46.095187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the first 5 and the last 5 rows of the train dataset to have an idea of how the data is presented.","metadata":{"ExecuteTime":{"start_time":"2023-04-02T19:29:29.986897Z","end_time":"2023-04-02T19:29:30.040233Z"}}},{"cell_type":"code","source":"df_train.head()","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:33.731111Z","end_time":"2023-04-06T00:25:33.832429Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:46.872886Z","iopub.execute_input":"2023-04-09T23:02:46.873248Z","iopub.status.idle":"2023-04-09T23:02:46.889171Z","shell.execute_reply.started":"2023-04-09T23:02:46.873219Z","shell.execute_reply":"2023-04-09T23:02:46.888057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.tail()","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:33.756858Z","end_time":"2023-04-06T00:25:33.833799Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:47.128446Z","iopub.execute_input":"2023-04-09T23:02:47.128797Z","iopub.status.idle":"2023-04-09T23:02:47.145163Z","shell.execute_reply.started":"2023-04-09T23:02:47.128769Z","shell.execute_reply":"2023-04-09T23:02:47.143798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the first and last 5 columns, there are some NaN data to clean.","metadata":{}},{"cell_type":"markdown","source":"Get the shape of train and test dataset","metadata":{}},{"cell_type":"code","source":"print('The shape of the train dataset and test dataset is:', df_train.shape, df_test.shape)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:33.776584Z","end_time":"2023-04-06T00:25:33.842229Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:47.617178Z","iopub.execute_input":"2023-04-09T23:02:47.617528Z","iopub.status.idle":"2023-04-09T23:02:47.624541Z","shell.execute_reply.started":"2023-04-09T23:02:47.617501Z","shell.execute_reply":"2023-04-09T23:02:47.622806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the basic datasets information to check the data types.","metadata":{}},{"cell_type":"code","source":"print(\"Train Info\")\ndf_train.info()","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:33.793349Z","end_time":"2023-04-06T00:25:34.210084Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:47.953323Z","iopub.execute_input":"2023-04-09T23:02:47.953666Z","iopub.status.idle":"2023-04-09T23:02:47.979806Z","shell.execute_reply.started":"2023-04-09T23:02:47.953639Z","shell.execute_reply":"2023-04-09T23:02:47.978811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Test Info\")\ndf_test.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:48.095749Z","iopub.execute_input":"2023-04-09T23:02:48.096082Z","iopub.status.idle":"2023-04-09T23:02:48.110447Z","shell.execute_reply.started":"2023-04-09T23:02:48.096057Z","shell.execute_reply":"2023-04-09T23:02:48.108314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Data Types\nFrom the above:\n\nThere are 8 Categorical features:\n\ntrip:                Ordinal category<br>\ndayPart:             Binary category<br>\nexWeatherTag:        Binary category<br>\noriginLocation:      Ordinal category<br>\ntype:                Binary category<br>\npackageType:         Ordinal category<br>\ncarrier:             Nominal category<br>\ndestinationLocation: Ordinal category\n\nThere is 1 Date type\n\nThere are 3 numeric features:\nweight:   Numeric Discret<br>\ncost:     Numeric Continuous<br>\ndistance: Numeric Continuous<br>","metadata":{}},{"cell_type":"markdown","source":"[Back to Table of Contents](#0.1)\n## Data Validation & Cleansing <a class=\"anchor\" id=\"6\"></a>","metadata":{}},{"cell_type":"markdown","source":"#### Describe\n\nInitial analysis to identify minimum, maximum, standard, and quartile values for each column. There are important data to take into account, such as:\n- There are two columns with unique value.\n- The mean price is 80.97 for every delivery. However, the min and max prices show possible outliers affecting the mean.\n- Possibly there are misleading values according to the number of records in the columns, type, packagetype and exweathertag.","metadata":{}},{"cell_type":"code","source":"df_train.describe(include=\"all\").T","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:34.34978Z","end_time":"2023-04-06T00:25:34.875991Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:48.830854Z","iopub.execute_input":"2023-04-09T23:02:48.831176Z","iopub.status.idle":"2023-04-09T23:02:48.912052Z","shell.execute_reply.started":"2023-04-09T23:02:48.83115Z","shell.execute_reply":"2023-04-09T23:02:48.909975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe(include=\"all\").T","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:34.414002Z","end_time":"2023-04-06T00:25:34.875991Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:49.007735Z","iopub.execute_input":"2023-04-09T23:02:49.008084Z","iopub.status.idle":"2023-04-09T23:02:49.042722Z","shell.execute_reply.started":"2023-04-09T23:02:49.008055Z","shell.execute_reply":"2023-04-09T23:02:49.04191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Date attribute is transformed\n\nThe date column is changed to date format, the Day,Month and epoch time columns are also created. ","metadata":{}},{"cell_type":"code","source":"# copy of the data train and data test are created\ndf_train_date=  df_train.copy()\ndf_test_date=  df_test.copy()\n\n# Set Date column as a datetime data type\n# Train Dataset\ndf_train_date[\"date\"] = df_train[\"date\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\ndf_train_date[\"date\"] = df_train[\"date\"].astype(\"datetime64[ns]\")\ndf_train_dat = np.vstack(df_train['date'].astype(str).apply(lambda x:list(map(int,x.split('-')))).values)\n\ndf_train_date['Year'] = df_train_dat[:,0]\ndf_train_date['Monthn']= df_train_dat[:,1]\ndf_train_date['Dayn'] = df_train_dat[:,2]\ndf_train_date['Day'] = df_train_date[\"date\"].dt.day_name()\ndf_train_date['Month'] = df_train_date[\"date\"].dt.month_name()\ndf_train_date['epoch_times'] = df_train_date['date'].apply(lambda x: date_to_epoch(x))\n\n# Test Dataset\ndf_test_date[\"date\"] = df_test[\"date\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\ndf_test_date[\"date\"] = df_test[\"date\"].astype(\"datetime64[ns]\")\ndf_test_dat = np.vstack(df_test['date'].astype(str).apply(lambda x:list(map(int,x.split('-')))).values)\n\ndf_test_date['Year'] = df_test_dat[:,0]\ndf_test_date['Monthn']= df_test_dat[:,1]\ndf_test_date['Dayn'] = df_test_dat[:,2]\ndf_test_date['Day'] = df_test_date[\"date\"].dt.day_name()\ndf_test_date['Month'] = df_test_date[\"date\"].dt.month_name()\ndf_test_date['epoch_times'] = df_test_date['date'].apply(lambda x: date_to_epoch(x))\n","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:33.869039Z","end_time":"2023-04-06T00:25:34.87399Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:49.315584Z","iopub.execute_input":"2023-04-09T23:02:49.315966Z","iopub.status.idle":"2023-04-09T23:02:50.276244Z","shell.execute_reply.started":"2023-04-09T23:02:49.315939Z","shell.execute_reply":"2023-04-09T23:02:50.274738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The change made is verified","metadata":{}},{"cell_type":"code","source":"df_train_date","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:34.319506Z","end_time":"2023-04-06T00:25:34.875991Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:50.279299Z","iopub.execute_input":"2023-04-09T23:02:50.27969Z","iopub.status.idle":"2023-04-09T23:02:50.310204Z","shell.execute_reply.started":"2023-04-09T23:02:50.279653Z","shell.execute_reply":"2023-04-09T23:02:50.308341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Duplicate Values <a class=\"anchor\" id=\"6.1\"></a>\nNo duplicate values\n","metadata":{}},{"cell_type":"code","source":"df_train_date[df_train_date[\"trip\"].isin(df_train_date[df_train_date.duplicated()][\"trip\"].values)]","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:34.721189Z","end_time":"2023-04-06T00:25:35.15463Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:50.312285Z","iopub.execute_input":"2023-04-09T23:02:50.312596Z","iopub.status.idle":"2023-04-09T23:02:50.375079Z","shell.execute_reply.started":"2023-04-09T23:02:50.312569Z","shell.execute_reply":"2023-04-09T23:02:50.373336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Unique Values and typographic error <a class=\"anchor\" id=\"6.2\"></a>\n\nThere are no typographical errors or redundancy in the name of the variables. It is observed the column type and packagetype have only 1 level or value, so they must be taken into consideration and evaluated.","metadata":{}},{"cell_type":"code","source":"cols_cat = ['dayPart', 'exWeatherTag', 'originLocation','destinationLocation', 'carrier', 'type', 'packageType']\nfor col in cols_cat:\n    print(f'Column {col}:{df_train_date[col].nunique()} categories')\n    print(df_train_date[col].value_counts().index.tolist())\n","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:34.749598Z","end_time":"2023-04-06T00:25:35.15463Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:50.402416Z","iopub.execute_input":"2023-04-09T23:02:50.402785Z","iopub.status.idle":"2023-04-09T23:02:50.441114Z","shell.execute_reply.started":"2023-04-09T23:02:50.402757Z","shell.execute_reply":"2023-04-09T23:02:50.439316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Identify Outliers <a class=\"anchor\" id=\"6.3\"></a>\n\nThe identification of outliers is carried out by means of box plots and the histogram for the numerical variables.\nBoxplots are a useful graphical tool for identifying outliers,show the distribution of data in terms of the median, quartiles, and extreme values.","metadata":{"ExecuteTime":{"end_time":"2023-04-01T11:01:13.200431Z","start_time":"2023-04-01T11:01:12.618968Z"}}},{"cell_type":"code","source":"cols_num = ['cost','distance','weight']\niden_outliers(df_train_date,cols_num)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:34.79836Z","end_time":"2023-04-06T00:25:35.579315Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:50.823124Z","iopub.execute_input":"2023-04-09T23:02:50.823494Z","iopub.status.idle":"2023-04-09T23:02:51.165868Z","shell.execute_reply.started":"2023-04-09T23:02:50.823466Z","shell.execute_reply":"2023-04-09T23:02:51.164119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution plots serve to complement the information provided by a boxplot. A distribution plot shows the shape and symmetry of the data distribution, which can be helpful in identifying patterns that are not apparent on a boxplot.","metadata":{}},{"cell_type":"code","source":"Graphs.distribution_plot(df_train_date,cols_num)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:51.183511Z","iopub.execute_input":"2023-04-09T23:02:51.183865Z","iopub.status.idle":"2023-04-09T23:02:52.453265Z","shell.execute_reply.started":"2023-04-09T23:02:51.183819Z","shell.execute_reply":"2023-04-09T23:02:52.451546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Within the analysis developed at this point of the numerical variables Cost, distance and weight, the following observations can be made:\n\nCost\n\n###### - The mean (80.97) is greater than the median (46.47), which suggests that the distribution be skewed to the right.\n###### - The mode of the distribution is 63, which means that this value is the most common in the sample. This value is not equal to the mean or median, which also suggests an asymmetry in the distribution.\n###### - The standard deviation (180.71) is relatively high compared to the mean. It may have outliers.","metadata":{}},{"cell_type":"code","source":"print(\"Analysis cost\")\noutliers_analysis(\"cost\")","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:36.253033Z","end_time":"2023-04-06T00:25:36.899379Z"},"execution":{"iopub.status.busy":"2023-04-09T23:02:52.45604Z","iopub.execute_input":"2023-04-09T23:02:52.457772Z","iopub.status.idle":"2023-04-09T23:02:52.469358Z","shell.execute_reply.started":"2023-04-09T23:02:52.457731Z","shell.execute_reply":"2023-04-09T23:02:52.467592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Weight\n\n###### - The mean (42.29) is greater than the median (25), which suggests that the distribution be skewed to the right.\n###### - The mode (mode) is much smaller than the mean and median. which also suggests an asymmetry in the distribution. \n###### - The standard deviation (180.71) is relatively high compared to the mean.It may have outliers.","metadata":{}},{"cell_type":"code","source":"print(\"Analysis weight\")\noutliers_analysis(\"weight\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:55.164251Z","iopub.execute_input":"2023-04-09T23:02:55.165199Z","iopub.status.idle":"2023-04-09T23:02:55.174444Z","shell.execute_reply.started":"2023-04-09T23:02:55.165164Z","shell.execute_reply":"2023-04-09T23:02:55.172295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distance\n\n###### - For this case, the data suggest that most of the samples are within the range of 1,000 to 3,000 kilometers, with a mean and median that are close to that range. The fact that the mode is also in this range indicates that there is a high concentration of values around this distance.","metadata":{}},{"cell_type":"code","source":"print(\"Analysis distance\")\noutliers_analysis(\"distance\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:02:55.722426Z","iopub.execute_input":"2023-04-09T23:02:55.722745Z","iopub.status.idle":"2023-04-09T23:02:55.73358Z","shell.execute_reply.started":"2023-04-09T23:02:55.722718Z","shell.execute_reply":"2023-04-09T23:02:55.731915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### As we see above, Cost, Distance and Weight column has outlier. But these all columns are related to target variable so we can not remove it.","metadata":{}},{"cell_type":"markdown","source":"#### Ouliers imputation\n\nThis part is for experimentation.\n\nBased on general observations, we might consider using outlier detection techniques that focus on identifying outliers in the right tail of the distribution for the cost, and weight columns, since the mean is greater than the median and the distribution is skewed to the right. Therefore, the technique \"interquartile range rule (IQR)\" could be used. This technique is useful when the distribution is not symmetric and has long tails in one direction.\n","metadata":{}},{"cell_type":"code","source":"df_train_without_Outliers =  df_train_date.copy()","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:36.270487Z","end_time":"2023-04-06T00:25:36.899379Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:02:56.868864Z","iopub.execute_input":"2023-04-09T23:02:56.86965Z","iopub.status.idle":"2023-04-09T23:02:56.879592Z","shell.execute_reply.started":"2023-04-09T23:02:56.869598Z","shell.execute_reply":"2023-04-09T23:02:56.877591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We called the function to find the outliers with the IQR technique","metadata":{}},{"cell_type":"code","source":"outliers_cost = iqr_outliers(df_train_without_Outliers[\"cost\"])\noutliers_weight = iqr_outliers(df_train_without_Outliers[\"weight\"])","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:36.285439Z","end_time":"2023-04-06T00:25:37.001567Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:02:58.261152Z","iopub.execute_input":"2023-04-09T23:02:58.261494Z","iopub.status.idle":"2023-04-09T23:02:58.3024Z","shell.execute_reply.started":"2023-04-09T23:02:58.261465Z","shell.execute_reply":"2023-04-09T23:02:58.301485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The imputation of the outliers was made","metadata":{}},{"cell_type":"code","source":"#calculate the median\nmedian_cost = np.median(df_train_without_Outliers['cost'])\nmedian_cost = np.median(df_train_without_Outliers['weight'])\n# Impute outliers with the median\ndf_train_without_Outliers['cost'].replace(outliers_cost, median_cost, inplace=True)\ndf_train_without_Outliers['weight'].replace(outliers_weight, median_cost,inplace=True)\n\n","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:36.316334Z","end_time":"2023-04-06T00:25:37.298907Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:02:58.728559Z","iopub.execute_input":"2023-04-09T23:02:58.728939Z","iopub.status.idle":"2023-04-09T23:02:59.182638Z","shell.execute_reply.started":"2023-04-09T23:02:58.72891Z","shell.execute_reply":"2023-04-09T23:02:59.180738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A distribution graph was made to see the behavior after imputing the outliers.","metadata":{}},{"cell_type":"code","source":"Graphs.distribution_plot(df_train_without_Outliers,cols_num)","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:36.678084Z","end_time":"2023-04-06T00:25:38.123297Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:02:59.423364Z","iopub.execute_input":"2023-04-09T23:02:59.423706Z","iopub.status.idle":"2023-04-09T23:03:00.396941Z","shell.execute_reply.started":"2023-04-09T23:02:59.423675Z","shell.execute_reply":"2023-04-09T23:03:00.395013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Missing Values <a class=\"anchor\" id=\"6.4\"></a>\n\nThis step identifies the percentage and quantity of missing values for each column. For this case, the columns type, packagetype and exWeathertag have 93.52%, 90.90% and 87.41% of missing values.","metadata":{"ExecuteTime":{"end_time":"2023-04-01T11:01:49.365142Z","start_time":"2023-04-01T11:01:49.345509Z"}}},{"cell_type":"code","source":"# Missing Values train\nmissing_value(df_test_date)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:38.099378Z","end_time":"2023-04-06T00:25:38.161215Z"},"execution":{"iopub.status.busy":"2023-04-09T23:03:00.443733Z","iopub.execute_input":"2023-04-09T23:03:00.44411Z","iopub.status.idle":"2023-04-09T23:03:00.465707Z","shell.execute_reply.started":"2023-04-09T23:03:00.444081Z","shell.execute_reply":"2023-04-09T23:03:00.462706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing Values train\nmissing_value(df_train_date)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:38.110341Z","end_time":"2023-04-06T00:25:38.469819Z"},"execution":{"iopub.status.busy":"2023-04-09T23:03:00.669327Z","iopub.execute_input":"2023-04-09T23:03:00.669693Z","iopub.status.idle":"2023-04-09T23:03:00.714725Z","shell.execute_reply.started":"2023-04-09T23:03:00.669665Z","shell.execute_reply":"2023-04-09T23:03:00.713011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function heatmap_nulls is created to show a heat map about nulls in all data frames. With this process, the goal is to identify the columns that are not relevant to the business problem.","metadata":{"ExecuteTime":{"end_time":"2023-04-01T11:01:57.373219Z","start_time":"2023-04-01T11:01:56.842117Z"}}},{"cell_type":"code","source":"Graphs(\"Quantity Nulls per Column\").heatmap_nulls(df_train_date)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:38.17427Z","end_time":"2023-04-06T00:25:38.875585Z"},"execution":{"iopub.status.busy":"2023-04-09T23:03:01.504786Z","iopub.execute_input":"2023-04-09T23:03:01.505155Z","iopub.status.idle":"2023-04-09T23:03:02.343189Z","shell.execute_reply.started":"2023-04-09T23:03:01.505128Z","shell.execute_reply":"2023-04-09T23:03:02.341079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A copy of the dataset was created to analyze the data","metadata":{}},{"cell_type":"code","source":"df_EDA = df_train_date.copy()","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:38.883559Z","end_time":"2023-04-06T00:25:39.003671Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:03:02.345212Z","iopub.execute_input":"2023-04-09T23:03:02.345507Z","iopub.status.idle":"2023-04-09T23:03:02.353171Z","shell.execute_reply.started":"2023-04-09T23:03:02.34548Z","shell.execute_reply":"2023-04-09T23:03:02.351918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 'Type' Column - We fill 'Not' in the missing value. Because according to the dataset discription we have only two category for the Type column.","metadata":{}},{"cell_type":"code","source":"# type variable\ndf_train_without_Outliers['type'] = df_train_without_Outliers['type'].fillna('Not')\ndf_train_date['type'] = df_train_date['type'].fillna('Not')\ndf_EDA['type'] = df_train_date['type'].fillna('Not')\ndf_test_date['type'] = df_test_date['type'].fillna('Not')","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:03:03.712036Z","iopub.execute_input":"2023-04-09T23:03:03.712351Z","iopub.status.idle":"2023-04-09T23:03:03.729566Z","shell.execute_reply.started":"2023-04-09T23:03:03.712327Z","shell.execute_reply":"2023-04-09T23:03:03.727501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 'packageType' Column - We fill 'standard' in the missing value.\n","metadata":{}},{"cell_type":"code","source":"# packagetype variable\ndf_train_without_Outliers['packageType'] = df_train_without_Outliers['packageType'].fillna('standar')\ndf_train_date['packageType'] = df_train_date['packageType'].fillna('standar')\ndf_EDA['packageType'] = df_train_date['packageType'].fillna('standar')\ndf_test_date['packageType'] = df_test_date['packageType'].fillna('standar')","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:03:05.75379Z","iopub.execute_input":"2023-04-09T23:03:05.754138Z","iopub.status.idle":"2023-04-09T23:03:05.771875Z","shell.execute_reply.started":"2023-04-09T23:03:05.754112Z","shell.execute_reply":"2023-04-09T23:03:05.770655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 'exWeatherTag' Column - According the dataset, only 6(June) month have 'heat' value in the dataset. And 12(december) month have 'snow' value in the dataset. So we assume that from the January to June we put 'heat' and from the July to December we put 'snow'.\n","metadata":{}},{"cell_type":"code","source":"# exweather variable\ndf_train_without_Outliers['exWeatherTag'] = df_train_date['Monthn'].apply(lambda x: 'heat' if x<= 6 else 'snow') \ndf_train_date['exWeatherTag'] = df_train_date['Monthn'].apply(lambda x: 'heat' if x<= 6 else 'snow') \ndf_EDA['exWeatherTag'] = df_train_date['Monthn'].apply(lambda x: 'heat' if x<= 6 else 'snow') \ndf_test_date['exWeatherTag'] = df_train_date['Monthn'].apply(lambda x: 'heat' if x<= 6 else 'snow')","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:38.899505Z","end_time":"2023-04-06T00:25:39.035299Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:03:06.592051Z","iopub.execute_input":"2023-04-09T23:03:06.592391Z","iopub.status.idle":"2023-04-09T23:03:06.632895Z","shell.execute_reply.started":"2023-04-09T23:03:06.592363Z","shell.execute_reply":"2023-04-09T23:03:06.630903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_EDA","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:03:06.826808Z","iopub.execute_input":"2023-04-09T23:03:06.82716Z","iopub.status.idle":"2023-04-09T23:03:06.858565Z","shell.execute_reply.started":"2023-04-09T23:03:06.827132Z","shell.execute_reply":"2023-04-09T23:03:06.856826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[Back to Table of Contents](#0.1)\n###  Exploration Data Analysis <a class=\"anchor\" id=\"7\"></a>","metadata":{}},{"cell_type":"markdown","source":"Convert categorical variables to numeric","metadata":{}},{"cell_type":"code","source":"cat_format = ['dayPart','exWeatherTag','originLocation','destinationLocation','type','packageType','carrier']\nfor var in cat_format:\n    format_data_etiqueta(df_EDA,var)\n    format_data_etiqueta(df_EDA,var)\n","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:38.961809Z","end_time":"2023-04-06T00:25:39.391819Z"},"execution":{"iopub.status.busy":"2023-04-09T23:03:07.729715Z","iopub.execute_input":"2023-04-09T23:03:07.730083Z","iopub.status.idle":"2023-04-09T23:03:07.830092Z","shell.execute_reply.started":"2023-04-09T23:03:07.730055Z","shell.execute_reply":"2023-04-09T23:03:07.828329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Correlation of Features\nAccording to the correlation matrix of the dataset, different variables with high correlations can be evidenced, such as: the distance variable with originLocation and destinationLocation. With respect to our target, this has a correlation of 0.86 with the weight and -0.66 with packageType, also a minor correction of 0.23 with the carrier variable.","metadata":{}},{"cell_type":"code","source":"Graphs(\"Correlation of Features\").heat_map_corr(df_EDA.corr())","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:03:10.295168Z","iopub.execute_input":"2023-04-09T23:03:10.295502Z","iopub.status.idle":"2023-04-09T23:03:11.28954Z","shell.execute_reply.started":"2023-04-09T23:03:10.295474Z","shell.execute_reply":"2023-04-09T23:03:11.288401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot a list of the variables from highest to lowest correlation with the target","metadata":{}},{"cell_type":"code","source":"corr = df_EDA.corr()\ncorr[['cost']].sort_values(by = 'cost',ascending = False)\\\n.style.background_gradient()","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:39.393814Z","end_time":"2023-04-06T00:25:39.486811Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:03:11.84987Z","iopub.execute_input":"2023-04-09T23:03:11.850239Z","iopub.status.idle":"2023-04-09T23:03:11.884633Z","shell.execute_reply.started":"2023-04-09T23:03:11.850212Z","shell.execute_reply":"2023-04-09T23:03:11.883416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Graph: Distance and (Mean cost)\nThe graph is made in order to know the relationship between distance and cost. As can be seen, the variation of the distance does not have a great significance within the cost.","metadata":{}},{"cell_type":"code","source":"df_graph = df_train_date[[\"cost\", \"distance\"]]\ndf_graph = df_graph.groupby([ \"distance\"], as_index=False)[\"cost\"].mean()\nGraphs(\"Distance (Mean cost) by year\", bar_fontsize=7, x_rotation=0, bar_rotation=0,\n              column_huge_name=\"Year\", legend_loc=\"best\").bar_plot(df_graph['distance'],\n                                                                           df_graph['cost'],\n                                                                           \"distance\", \"cost\")","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:39.490101Z","end_time":"2023-04-06T00:25:39.685782Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:03:13.045626Z","iopub.execute_input":"2023-04-09T23:03:13.045973Z","iopub.status.idle":"2023-04-09T23:03:13.362404Z","shell.execute_reply.started":"2023-04-09T23:03:13.045945Z","shell.execute_reply":"2023-04-09T23:03:13.360233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Graph: Media Cost by Month and year\nIn order to obtain data about the variation of the cost in the course of the months and years, the graph is made. The graph shows a variation in the price in the different months, being January the month with the highest cost, in turn it can be seen that as the years go by, the cost increases between 2 and 3%","metadata":{}},{"cell_type":"code","source":"df_graph = df_train_date[[\"cost\", \"Month\",'Year']]\ndf_graph = df_graph.groupby([\"Month\",'Year'], as_index=False)[\"cost\"].mean()\ndf_graph.sort_values(by=\"Month\", inplace=True, key=lambda x: x.map(order_months))\nfig = px.line(df_graph, x=\"Month\", y=\"cost\", color = 'Year')\nfig.update_layout(title=dict(text=\"Cost (mean) per year\", font=dict(size=20), yref='paper'))\nfig.show()","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:25:39.660343Z","end_time":"2023-04-06T00:25:39.782956Z"},"execution":{"iopub.status.busy":"2023-04-09T23:03:14.871084Z","iopub.execute_input":"2023-04-09T23:03:14.871439Z","iopub.status.idle":"2023-04-09T23:03:14.94724Z","shell.execute_reply.started":"2023-04-09T23:03:14.87141Z","shell.execute_reply":"2023-04-09T23:03:14.945743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cost (mean) vs dayPart per Year\n\nThis graph is highlighted in order to observe the relationship over the years between the cost and daypart variables. There is no evidence of a significant relationship or a variation in the cost more than the one that normally increases per year.\n","metadata":{}},{"cell_type":"code","source":"df_graph = df_train_date[[\"cost\", \"dayPart\",'Year']]\ndf_graph = df_graph.groupby([\"dayPart\",'Year'], as_index=False)[\"cost\"].mean()\ndf_graph.sort_values(by=\"dayPart\", inplace=True, key=lambda x: x.map(order_months))\nfig = px.bar(df_graph, x=\"dayPart\", y=\"cost\", color = 'Year')\nfig.update_layout(title=dict(text=\"Cost (mean) vs dayPart per Year\", font=dict(size=20), yref='paper'))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:03:16.383237Z","iopub.execute_input":"2023-04-09T23:03:16.383574Z","iopub.status.idle":"2023-04-09T23:03:16.452942Z","shell.execute_reply.started":"2023-04-09T23:03:16.383538Z","shell.execute_reply":"2023-04-09T23:03:16.451977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Carrier (Mean cost) by Month \nCost of the Carrier Type 'C' is very high as compared to the other Types.\nThe behavior of the variables in the different months is similar with the exception of variable C, which stands out for having the highest cost mean.","metadata":{}},{"cell_type":"code","source":"df_graph = df_EDA[[\"cost\", \"carrier\", \"Month\"]]\ndf_graph = df_graph.groupby([\"carrier\", \"Month\"], as_index=False)[\"cost\"].mean()\nGraphs(\"Carrier (Mean cost) by Month\", bar_fontsize=7, x_rotation=0, bar_rotation=0,\n              column_huge_name=\"Year\", legend_loc=\"best\").bar_plot(df_graph['Month'],\n                                                                           df_graph['cost'],\n                                                                           \"Month\", \"cost\", df_graph[\"carrier\"],order_months)","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:41.960518Z","end_time":"2023-04-06T00:25:42.480703Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:03:17.942181Z","iopub.execute_input":"2023-04-09T23:03:17.943361Z","iopub.status.idle":"2023-04-09T23:03:18.431491Z","shell.execute_reply.started":"2023-04-09T23:03:17.943315Z","shell.execute_reply":"2023-04-09T23:03:18.430501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Weight(mean) vs packageType\nAccording to the graph, there is a relationship between the packageType and weight column where the highest concentration of data is found in the TT variable with an average weight of 302.","metadata":{}},{"cell_type":"code","source":"df_graph = df_train_date[[\"weight\", \"packageType\"]]\ndf_graph = df_graph.groupby([\"packageType\"], as_index=False)[\"weight\"].mean()\nGraphs(\"Weight vs packageType\").bar_plot(df_graph['packageType'], df_graph['weight'], \"packageType\", \"weight\")","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:42.474723Z","end_time":"2023-04-06T00:25:42.749096Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:03:19.903283Z","iopub.execute_input":"2023-04-09T23:03:19.904742Z","iopub.status.idle":"2023-04-09T23:03:20.054561Z","shell.execute_reply.started":"2023-04-09T23:03:19.904678Z","shell.execute_reply":"2023-04-09T23:03:20.053452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Distribution Between Cost and Weight by type\nThis graph is made in order to see the relationship between the variables weight cost and type. In this graph, two different behaviors are observed, one linear and the other exponential, the latter starting from the weight of 200. In turn, it is observed that the type variable has a fairly close relationship between the exponential graph. From the above, it can be deduced that there is a close relationship between the weight and cost variable without mentioning that the type variable has a particular behavior in the price increase.","metadata":{}},{"cell_type":"code","source":"fig_px = px.scatter(df_train_date, \"weight\", \"cost\",color = \"type\",title=\"Distribution Between Cost and Weight by type \")\nfig_px.update_coloraxes(showscale=False)\nfig_px.show()","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:42.742119Z","end_time":"2023-04-06T00:25:43.137815Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:03:21.590566Z","iopub.execute_input":"2023-04-09T23:03:21.591677Z","iopub.status.idle":"2023-04-09T23:03:21.661669Z","shell.execute_reply.started":"2023-04-09T23:03:21.591638Z","shell.execute_reply":"2023-04-09T23:03:21.660594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Carrier (Mean cost) by exWeatherTag.\nCost of the Carrier Type ‘C’ is very high as compared to the other Types. Also the relation of weathertag is not changing with the carrier column ","metadata":{}},{"cell_type":"code","source":"df_graph = df_train_date[[\"cost\", \"exWeatherTag\", \"carrier\"]]\ndf_graph = df_graph.groupby([\"exWeatherTag\", \"carrier\"], as_index=False)[\"cost\"].mean()\nGraphs(\"carrier (Mean cost) by exWeatherTag\", bar_fontsize=9, x_rotation=0, bar_rotation=0,\n              column_huge_name=\"Weather\", legend_loc=\"best\").bar_plot(df_graph['carrier'],\n                                                                           df_graph['cost'],\n                                                                           \"carrier\", \"cost\", df_graph[\"exWeatherTag\"])","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:25:43.693263Z","end_time":"2023-04-06T00:25:43.969903Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:03:24.681252Z","iopub.execute_input":"2023-04-09T23:03:24.681649Z","iopub.status.idle":"2023-04-09T23:03:24.889345Z","shell.execute_reply.started":"2023-04-09T23:03:24.681572Z","shell.execute_reply":"2023-04-09T23:03:24.888085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_EDA","metadata":{"collapsed":false,"ExecuteTime":{"start_time":"2023-04-06T00:26:03.136977Z","end_time":"2023-04-06T00:26:03.253345Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-04-09T23:03:24.906001Z","iopub.execute_input":"2023-04-09T23:03:24.906368Z","iopub.status.idle":"2023-04-09T23:03:24.911207Z","shell.execute_reply.started":"2023-04-09T23:03:24.906339Z","shell.execute_reply":"2023-04-09T23:03:24.910132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The initial correlation is created to understand which fields are valuable for solving the problem definition (cost). Initially, it is notable that year, parking assist, alloy wheels, and transmission have a bit of relevance. Nevertheless, the other columns are categorical. Those could be more accurate once they become to numerical. As a result, it is necessary cleaning the data.","metadata":{}},{"cell_type":"markdown","source":"[Back to Table of Contents](#0.1)\n### Machine Learning Techniques <a class=\"anchor\" id=\"8\"></a>\n\nThere is a defined linear regression,Random Fores Regressor,Gradient Boosting Regressor and XGB Regressor to check the accuracy.","metadata":{}},{"cell_type":"markdown","source":"##### We have dropped the unnecessary column for the training the model","metadata":{}},{"cell_type":"code","source":"X1 = df_train_date.drop(columns=['cost','date','trip','Day','Month','Dayn','Monthn','Year','dayPart',\n                                 'Dayn','Monthn','type','originLocation','destinationLocation'])\ny1 = df_train_date['cost']\nt1 = df_test_date.drop(columns=['date','trip','Day','Month','Year','dayPart',\n                                'Dayn','Monthn','Dayn','Monthn','type','originLocation','destinationLocation'])","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:31:53.30938Z","end_time":"2023-04-06T00:31:53.362706Z"},"execution":{"iopub.status.busy":"2023-04-09T23:24:45.393964Z","iopub.execute_input":"2023-04-09T23:24:45.394298Z","iopub.status.idle":"2023-04-09T23:24:45.404935Z","shell.execute_reply.started":"2023-04-09T23:24:45.394272Z","shell.execute_reply":"2023-04-09T23:24:45.403896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the dataframe ","metadata":{}},{"cell_type":"code","source":"X1","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:31:16.891954Z","end_time":"2023-04-06T00:31:16.98367Z"},"execution":{"iopub.status.busy":"2023-04-09T23:24:47.329796Z","iopub.execute_input":"2023-04-09T23:24:47.331072Z","iopub.status.idle":"2023-04-09T23:24:47.347219Z","shell.execute_reply.started":"2023-04-09T23:24:47.331014Z","shell.execute_reply":"2023-04-09T23:24:47.345625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Create the dummy variable of the categorical data.","metadata":{}},{"cell_type":"code","source":"#Format Data\n\n#Train\ncat_format_onehot = ['carrier','exWeatherTag','packageType']\n#cat_format_onehot = ['originLocation','destinationLocation','type','packageType','carrier','exWeatherTag']\nfor var in cat_format_onehot:\n    X1 = merge_dummy_data(X1,var)\n\n#Test\nfor var in cat_format_onehot:\n    t1 = merge_dummy_data(t1,var)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:31:55.309099Z","end_time":"2023-04-06T00:31:55.38387Z"},"execution":{"iopub.status.busy":"2023-04-09T23:25:12.794487Z","iopub.execute_input":"2023-04-09T23:25:12.794801Z","iopub.status.idle":"2023-04-09T23:25:12.832233Z","shell.execute_reply.started":"2023-04-09T23:25:12.794773Z","shell.execute_reply":"2023-04-09T23:25:12.830897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:25:13.851755Z","iopub.execute_input":"2023-04-09T23:25:13.852126Z","iopub.status.idle":"2023-04-09T23:25:13.869257Z","shell.execute_reply.started":"2023-04-09T23:25:13.852099Z","shell.execute_reply":"2023-04-09T23:25:13.867631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Train and Test Split <a class=\"anchor\" id=\"8.2\"></a>","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.25, random_state = 10)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:31:57.186795Z","end_time":"2023-04-06T00:31:57.269027Z"},"execution":{"iopub.status.busy":"2023-04-09T23:25:16.483381Z","iopub.execute_input":"2023-04-09T23:25:16.48373Z","iopub.status.idle":"2023-04-09T23:25:16.493623Z","shell.execute_reply.started":"2023-04-09T23:25:16.483703Z","shell.execute_reply":"2023-04-09T23:25:16.492054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the number of columns","metadata":{}},{"cell_type":"code","source":"print('The shape of the train dataset and test dataset is:', X1.shape, t1.shape)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:31:57.831856Z","end_time":"2023-04-06T00:31:57.907099Z"},"execution":{"iopub.status.busy":"2023-04-09T23:25:19.063725Z","iopub.execute_input":"2023-04-09T23:25:19.064303Z","iopub.status.idle":"2023-04-09T23:25:19.06921Z","shell.execute_reply.started":"2023-04-09T23:25:19.064267Z","shell.execute_reply":"2023-04-09T23:25:19.068552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lineal Regression <a class=\"anchor\" id=\"8.2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Linear regression is a statistical technique used to measure the relationship between a dependent variable and one or more independent variables. It is used to create a model that can be used to make predictions, such as predicting a company’s stock price based on various factors. Linear regression is useful for understanding the strength and direction of the relationship between variables and can help identify which independent variables are most strongly associated with the dependent variable.","metadata":{}},{"cell_type":"markdown","source":"##### Initial Parameters\n\nFirstly, it is necessary to identify what are the best hyperparameter to improve our model.","metadata":{}},{"cell_type":"code","source":"param_grid_LN = {'fit_intercept': [True, False],\n              'copy_X': [True, False],\n              'n_jobs': [1, 2, 4, 8, 16,32,48,200],\n              'positive': [True, False]}\n\nLN = LinearRegression()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:25:22.536706Z","iopub.execute_input":"2023-04-09T23:25:22.537078Z","iopub.status.idle":"2023-04-09T23:25:22.542558Z","shell.execute_reply.started":"2023-04-09T23:25:22.537049Z","shell.execute_reply":"2023-04-09T23:25:22.54153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Grid Search\nGrid search involves specifying a grid of potential values for each hyperparameter, and then exhaustively searching through the grid to determine the combination of hyperparameters that yields the best performance of the model on a validation set.","metadata":{}},{"cell_type":"code","source":"best_estimators_LN = grid_search_cv(LN, param_grid_LN, X_train, y_train)\nLNG = best_estimators_LN\nLNG.fit(X_train, y_train)\nLNG_y_pred = LNG.predict(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:25:24.442867Z","iopub.execute_input":"2023-04-09T23:25:24.443363Z","iopub.status.idle":"2023-04-09T23:25:26.587613Z","shell.execute_reply.started":"2023-04-09T23:25:24.443337Z","shell.execute_reply":"2023-04-09T23:25:26.585687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Grid Search - Linear Regression')\nmodel_error(y_test,LNG_y_pred) \n","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:25:28.215084Z","iopub.execute_input":"2023-04-09T23:25:28.216189Z","iopub.status.idle":"2023-04-09T23:25:28.2235Z","shell.execute_reply.started":"2023-04-09T23:25:28.216152Z","shell.execute_reply":"2023-04-09T23:25:28.222166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normal Execution","metadata":{}},{"cell_type":"code","source":"reg = linearregresion(X_train, y_train )\nLN_y_pred = reg.predict(X_test)\nprint('Normal - Linear Regressor')\nmodel_error(y_test,LN_y_pred)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:07:36.53724Z","end_time":"2023-04-06T00:07:36.633586Z"},"execution":{"iopub.status.busy":"2023-04-09T23:25:30.526082Z","iopub.execute_input":"2023-04-09T23:25:30.526451Z","iopub.status.idle":"2023-04-09T23:25:30.545729Z","shell.execute_reply.started":"2023-04-09T23:25:30.526422Z","shell.execute_reply":"2023-04-09T23:25:30.544907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Importance\nIn this section, Transaction4, and SumTransactions have great relevance in the prediction. With this in mind, creating other groups to expand the combinations to improve the model is possible.","metadata":{}},{"cell_type":"code","source":"feature_importance_lineal(LNG, X_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:25:33.180588Z","iopub.execute_input":"2023-04-09T23:25:33.181552Z","iopub.status.idle":"2023-04-09T23:25:33.188597Z","shell.execute_reply.started":"2023-04-09T23:25:33.181522Z","shell.execute_reply":"2023-04-09T23:25:33.186937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_lineal(reg, X_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:25:34.996648Z","iopub.execute_input":"2023-04-09T23:25:34.996997Z","iopub.status.idle":"2023-04-09T23:25:35.003701Z","shell.execute_reply.started":"2023-04-09T23:25:34.996961Z","shell.execute_reply":"2023-04-09T23:25:35.002206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Export Results","metadata":{}},{"cell_type":"code","source":"LNG_y_pred_f = LNG.predict(t1)\nLN_y_pred_f = reg.predict(t1)\nexport_result(LNG_y_pred_f,df_test,\"Test15 LN - Gridsearch\")\nexport_result(LN_y_pred_f,df_test,\"Test15 LN - Normal\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:03:57.523159Z","iopub.execute_input":"2023-04-09T23:03:57.523871Z","iopub.status.idle":"2023-04-09T23:03:57.553222Z","shell.execute_reply.started":"2023-04-09T23:03:57.523771Z","shell.execute_reply":"2023-04-09T23:03:57.552365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Graph Error\nThe scatter graph of the error is made to know its behavior.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(12, 4))\naxes[0].scatter(y_test, LNG_y_pred)\naxes[1].scatter(y_test, LN_y_pred)\naxes[0].set_title('Linear Regression - Grid search', fontsize=12)\naxes[1].set_title('Linear Regression - Normal', fontsize=12)\naxes[0].set_xlabel('y_test', fontsize=12)\naxes[0].set_ylabel('y_pred', fontsize=12)\naxes[1].set_xlabel('y_test', fontsize=12)\naxes[1].set_ylabel('y_pred', fontsize=12)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:25:41.223911Z","iopub.execute_input":"2023-04-09T23:25:41.224274Z","iopub.status.idle":"2023-04-09T23:25:41.699886Z","shell.execute_reply.started":"2023-04-09T23:25:41.224245Z","shell.execute_reply":"2023-04-09T23:25:41.698797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Export Results\nIn this section, the results is exported to compare to kaggle competition\n\nKaggle Score: 0.59628\n<br/>\nLocal Score: 0.618680","metadata":{}},{"cell_type":"markdown","source":"### Random Fores Regressor <a class=\"anchor\" id=\"8.3\"></a>\nRandom Forest Regressor is a machine learning algorithm used to predict continuous values. It is commonly used to solve business problems such as predicting future prices or costs. Random Forest Regressor is a powerful algorithm that combines multiple decision trees to create an ensemble model. It can handle missing values, outliers, and high-dimensional data, making it suitable for a wide range of problems.","metadata":{}},{"cell_type":"markdown","source":"##### Initial Parameters\n\nFirstly, it is necessary to identify what are the best hyperparameter to improve our model.","metadata":{}},{"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2010, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# First create the base model to tune\nRFR = RandomForestRegressor()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:25:45.463131Z","iopub.execute_input":"2023-04-09T23:25:45.46347Z","iopub.status.idle":"2023-04-09T23:25:45.471307Z","shell.execute_reply.started":"2023-04-09T23:25:45.463443Z","shell.execute_reply":"2023-04-09T23:25:45.470246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Randomized Search CV","metadata":{}},{"cell_type":"code","source":"best_estimators = randomized_search_cv(RFR, random_grid, X_train, y_train)\nRFR_R = best_estimators\nRFR_R.fit(X_train, y_train)\nRFR_R_y_pred = RFR_R.predict(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:30:06.632726Z","iopub.execute_input":"2023-04-09T19:30:06.633093Z","iopub.status.idle":"2023-04-09T19:45:35.439722Z","shell.execute_reply.started":"2023-04-09T19:30:06.633065Z","shell.execute_reply":"2023-04-09T19:45:35.437804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Randomized Search - Random Fores Regressor')\nmodel_error(y_test,RFR_R_y_pred) ","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:51:27.268262Z","iopub.execute_input":"2023-04-09T19:51:27.268616Z","iopub.status.idle":"2023-04-09T19:51:27.279694Z","shell.execute_reply.started":"2023-04-09T19:51:27.268589Z","shell.execute_reply":"2023-04-09T19:51:27.276922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normal Execution","metadata":{}},{"cell_type":"code","source":"RFR_N = random_forest_regressor(X_train, y_train)\nRFR_N_y_pred = RFR_N.predict(X_test)\nprint('Normal - Random Fores Regressor')\nmodel_error(y_test,RFR_N_y_pred)","metadata":{"ExecuteTime":{"start_time":"2023-04-06T00:32:01.934292Z","end_time":"2023-04-06T00:32:08.22819Z"},"execution":{"iopub.status.busy":"2023-04-09T19:18:01.902031Z","iopub.execute_input":"2023-04-09T19:18:01.902599Z","iopub.status.idle":"2023-04-09T19:18:11.1399Z","shell.execute_reply.started":"2023-04-09T19:18:01.90255Z","shell.execute_reply":"2023-04-09T19:18:11.138213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Importance","metadata":{}},{"cell_type":"code","source":"feature_importance(RFR_R, X_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:54:31.751154Z","iopub.execute_input":"2023-04-09T19:54:31.751603Z","iopub.status.idle":"2023-04-09T19:54:31.948526Z","shell.execute_reply.started":"2023-04-09T19:54:31.751568Z","shell.execute_reply":"2023-04-09T19:54:31.946574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance(RFR_N, X_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:54:55.059755Z","iopub.execute_input":"2023-04-09T19:54:55.060329Z","iopub.status.idle":"2023-04-09T19:54:55.105481Z","shell.execute_reply.started":"2023-04-09T19:54:55.060275Z","shell.execute_reply":"2023-04-09T19:54:55.102948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Export Results","metadata":{}},{"cell_type":"code","source":"RFR_R_y_pred_f = RFR_R.predict(t1)\nRFR_N_y_pred_f = RFR_N.predict(t1)\nexport_result(RFR_R_y_pred_f,df_test,\"Test15 RFR - Randomized\")\nexport_result(RFR_N_y_pred_f,df_test,\"Test15 RFR - Normal\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:56:56.183859Z","iopub.execute_input":"2023-04-09T19:56:56.184468Z","iopub.status.idle":"2023-04-09T19:56:56.706692Z","shell.execute_reply.started":"2023-04-09T19:56:56.184412Z","shell.execute_reply":"2023-04-09T19:56:56.705181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Graph Error","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(16, 4))\naxes[0].scatter(y_test,RFR_R_y_pred)\naxes[1].scatter(y_test,RFR_N_y_pred)\naxes[0].set_title('Random Fores Regresso - Randomized Search CV', fontsize=12)\naxes[1].set_title('Random Fores Regresso - Normal', fontsize=12)\naxes[0].set_xlabel('y_test', fontsize=12)\naxes[0].set_ylabel('y_pred', fontsize=12)\naxes[1].set_xlabel('y_test', fontsize=12)\naxes[1].set_ylabel('y_pred', fontsize=12)\nfig.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting Regressor <a class=\"anchor\" id=\"8.4\"></a>\n\nGradient Boosting Regressor is a machine learning algorithm used to build regression models that are capable of capturing nonlinear relationships between the target variable and features. It can handle missing values, outliers, and high cardinality categorical features without any special preprocessing. Gradient Boosting Regressor is a powerful algorithm that can achieve high accuracy and has good usability for a wide range of problems.","metadata":{}},{"cell_type":"markdown","source":"##### Initial Parameters\n\nFirstly, it is necessary to identify what are the best hyperparameter to improve our model.","metadata":{}},{"cell_type":"code","source":"param_grid_GBR = {\n    'n_estimators': [50, 100,500],\n    'learning_rate': [0.01, 0.1, 1],\n    'max_depth': [2, 3, 4],\n    'min_samples_split': [2, 5, 10],\n}\n\n# First create the base model to tune\nGBR = GradientBoostingRegressor()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:03:19.513024Z","iopub.execute_input":"2023-04-09T19:03:19.513485Z","iopub.status.idle":"2023-04-09T19:03:19.520769Z","shell.execute_reply.started":"2023-04-09T19:03:19.513438Z","shell.execute_reply":"2023-04-09T19:03:19.518645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Grid search","metadata":{}},{"cell_type":"code","source":"bestest_GBR = grid_search_cv(GBR, param_grid_GBR, X_train, y_train)\nGBR_G = bestest_GBR\nGBR_G.fit(X_train, y_train)\nGBR_G_y_pred = GBR_G.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:03:21.007326Z","iopub.execute_input":"2023-04-09T19:03:21.008428Z","iopub.status.idle":"2023-04-09T19:08:39.834807Z","shell.execute_reply.started":"2023-04-09T19:03:21.008386Z","shell.execute_reply":"2023-04-09T19:08:39.833507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Grid Search - GBR Regression')\nmodel_error(y_test,GBR_G_y_pred) ","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:13:05.312242Z","iopub.execute_input":"2023-04-09T19:13:05.312645Z","iopub.status.idle":"2023-04-09T19:13:05.322125Z","shell.execute_reply.started":"2023-04-09T19:13:05.312607Z","shell.execute_reply":"2023-04-09T19:13:05.320458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normal Execution","metadata":{}},{"cell_type":"code","source":"GBR_N = GBR.fit(X_train, y_train)\nGBR_N_y_pred = GBR_N.predict(X_test)\nprint('Normal - GBR Regressor')\nmodel_error(y_test,GBR_N_y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:19:43.333325Z","iopub.execute_input":"2023-04-09T19:19:43.333722Z","iopub.status.idle":"2023-04-09T19:19:45.932095Z","shell.execute_reply.started":"2023-04-09T19:19:43.333686Z","shell.execute_reply":"2023-04-09T19:19:45.931071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Importance\nIn this section, Transaction4, and SumTransactions have great relevance in the prediction. With this in mind, creating other groups to expand the combinations to improve the model is possible.","metadata":{}},{"cell_type":"code","source":"feature_importance(GBR_G, X_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:18:48.80502Z","iopub.execute_input":"2023-04-09T19:18:48.805403Z","iopub.status.idle":"2023-04-09T19:18:48.81295Z","shell.execute_reply.started":"2023-04-09T19:18:48.805375Z","shell.execute_reply":"2023-04-09T19:18:48.811868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance(GBR_N, X_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:55:20.678753Z","iopub.execute_input":"2023-04-09T19:55:20.679156Z","iopub.status.idle":"2023-04-09T19:55:20.688326Z","shell.execute_reply.started":"2023-04-09T19:55:20.679111Z","shell.execute_reply":"2023-04-09T19:55:20.686886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Export Results","metadata":{}},{"cell_type":"code","source":"GBR_G_y_pred_f = GBR_G.predict(t1)\nGBR_N_y_pred_f = GBR_N.predict(t1)\nexport_result(GBR_G_y_pred_f,df_test,\"Test15 GBR - Gridsearch\")\nexport_result(GBR_N_y_pred_f,df_test,\"Test15 GBR - Normal\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:57:27.98304Z","iopub.execute_input":"2023-04-09T19:57:27.98353Z","iopub.status.idle":"2023-04-09T19:57:28.019815Z","shell.execute_reply.started":"2023-04-09T19:57:27.983487Z","shell.execute_reply":"2023-04-09T19:57:28.01866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Graph Error","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=3, figsize=(16, 4))\naxes[0].scatter(y_test,GBR_G_y_pred)\naxes[1].scatter(y_test,GBR_N_y_pred)\naxes[0].set_title('Gradient Boosting Regressor - Grid search', fontsize=12)\naxes[1].set_title('Gradient Boosting Regressor - Normal', fontsize=12)\naxes[0].set_xlabel('y_test', fontsize=12)\naxes[0].set_ylabel('y_pred', fontsize=12)\naxes[1].set_xlabel('y_test', fontsize=12)\naxes[1].set_ylabel('y_pred', fontsize=12)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T19:55:34.37537Z","iopub.execute_input":"2023-04-09T19:55:34.377366Z","iopub.status.idle":"2023-04-09T19:55:35.396875Z","shell.execute_reply.started":"2023-04-09T19:55:34.37727Z","shell.execute_reply":"2023-04-09T19:55:35.394524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGB Regressor  <a class=\"anchor\" id=\"8.5\"></a>\n\nXGBoost is also know as custom tree building algorithm. It is sequential model. Each sub tree is dependent on the outcome of the previous tree.\nXGBoost, which stands for Extreme Gradient Boosting, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems.","metadata":{}},{"cell_type":"markdown","source":"##### Initial Parameters\n\nFirstly, it is necessary to identify what are the best hyperparameter to improve our model.","metadata":{}},{"cell_type":"code","source":"param_grid_XGB = {   'learning_rate': [0.1, 0.01, 0.001],\n    'max_depth': [3, 5, 7],\n    'n_estimators': [50, 100, 500],\n    'subsample': [0.5, 0.7, 1.0],\n    'colsample_bytree': [0.5, 0.7, 1.0],\n    'gamma': [0, 0.1, 0.2],\n    'reg_alpha': [0, 0.1, 0.5],\n    'reg_lambda': [0.1, 1, 10]}\nXGB = XGBRegressor()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:25:52.107Z","iopub.execute_input":"2023-04-09T23:25:52.107332Z","iopub.status.idle":"2023-04-09T23:25:52.115015Z","shell.execute_reply.started":"2023-04-09T23:25:52.107306Z","shell.execute_reply":"2023-04-09T23:25:52.11372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Randomized Search CV","metadata":{}},{"cell_type":"code","source":"bestest_XGB = randomized_search_cv(XGB, param_grid_XGB, X_train, y_train)\nXGB_R = bestest_XGB\nXGB_R.fit(X_train, y_train)\nXGB_R_y_pred = XGB_R.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:31:22.067963Z","iopub.execute_input":"2023-04-09T23:31:22.068534Z","iopub.status.idle":"2023-04-09T23:32:38.009099Z","shell.execute_reply.started":"2023-04-09T23:31:22.068462Z","shell.execute_reply":"2023-04-09T23:32:38.008213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Randomized Search - XGB Regression')\nmodel_error(y_test,XGB_R_y_pred) ","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:32:41.588811Z","iopub.execute_input":"2023-04-09T23:32:41.590098Z","iopub.status.idle":"2023-04-09T23:32:41.597619Z","shell.execute_reply.started":"2023-04-09T23:32:41.590045Z","shell.execute_reply":"2023-04-09T23:32:41.596513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normal Execution","metadata":{}},{"cell_type":"code","source":"XGB_N = XGB.fit(X_train, y_train)\nXGB_N_y_pred = XGB_N.predict(X_test)\nprint('Normal - XGB Regressor')\nmodel_error(y_test,XGB_N_y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:32:50.711675Z","iopub.execute_input":"2023-04-09T23:32:50.712023Z","iopub.status.idle":"2023-04-09T23:32:52.675978Z","shell.execute_reply.started":"2023-04-09T23:32:50.711995Z","shell.execute_reply":"2023-04-09T23:32:52.67469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Importance","metadata":{}},{"cell_type":"code","source":"feature_importance(XGB_N, X_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:33:17.994289Z","iopub.execute_input":"2023-04-09T23:33:17.994687Z","iopub.status.idle":"2023-04-09T23:33:18.002897Z","shell.execute_reply.started":"2023-04-09T23:33:17.994659Z","shell.execute_reply":"2023-04-09T23:33:18.001486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance(XGB_R, X_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:33:19.952876Z","iopub.execute_input":"2023-04-09T23:33:19.9537Z","iopub.status.idle":"2023-04-09T23:33:19.962765Z","shell.execute_reply.started":"2023-04-09T23:33:19.953629Z","shell.execute_reply":"2023-04-09T23:33:19.961281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Export Results","metadata":{}},{"cell_type":"code","source":"XGB_R_y_pred_f = XGB_R.predict(t1)\nXGB_N_y_pred_f = XGB_N.predict(t1)\nexport_result(XGB_R_y_pred_f,df_test,\"Test18 XGB - Randomized\")\nexport_result(XGB_N_y_pred_f,df_test,\"Test18 XGB - Normal\")","metadata":{"execution":{"iopub.status.busy":"2023-04-09T23:33:27.539771Z","iopub.execute_input":"2023-04-09T23:33:27.54014Z","iopub.status.idle":"2023-04-09T23:33:27.564752Z","shell.execute_reply.started":"2023-04-09T23:33:27.540111Z","shell.execute_reply":"2023-04-09T23:33:27.564021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Graph Error","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(16, 4))\naxes[0].scatter(y_test,XGB_R_y_pred)\naxes[1].scatter(y_test,XGB_N_y_pred)\naxes[0].set_title('XGB Regressor - Grid search', fontsize=12)\naxes[1].set_title('XGB Regressor - Normal', fontsize=12)\naxes[0].set_xlabel('y_test', fontsize=12)\naxes[0].set_ylabel('y_pred', fontsize=12)\naxes[1].set_xlabel('y_test', fontsize=12)\naxes[1].set_ylabel('y_pred', fontsize=12)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-04-09T20:49:43.524304Z","iopub.execute_input":"2023-04-09T20:49:43.525282Z","iopub.status.idle":"2023-04-09T20:49:44.162808Z","shell.execute_reply.started":"2023-04-09T20:49:43.525223Z","shell.execute_reply":"2023-04-09T20:49:44.159894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}